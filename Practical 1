Aim: Data Wrangling, I Perform the following operations using Python on any open source dataset 

Code: 

import pandas as pd
import numpy as np

data = {
    "Name": ["Amit", "Priya", "Raj", "Sneha", "Vikram", "Ananya", "Rohan"],
    "Gender": ["Male", "Female", "Male", "Female", "Male", "Female", "Male"],
    "Marks": [85, 80, 78, 'Nan', 76, 82, 'Nan'],
    "Age": [20,21,22,23,24,25,26]
}
df = pd.DataFrame(data)
print(df)

df.mean()

cat=[]
con=[]
for i in df.columns:
    if(df[i].dtypes=="object"):
        cat.append(i)
else:
    con.append(i)
df

cat

con

c=avg=sum=0
for ele in df['Marks']:
    if str(ele).isnumeric():
        c+=1
        sum+=ele
if c>0:
    avg=sum/c
df=df.replace(to_replace='Nan', value=avg)
df   

df.isna().sum().plot(kind="bar")

df['Gender']=df['Gender'].map({'Male':0,'Female':1,}).astype(int)
df

df=df[df['Marks']>80]
df

df=df.drop(['Age'], axis=1)
df

data1 = {
    "Name": ["Amit", "Priya", "Raj", "Sneha", "Vikram", "Ananya", "Rohan"],
    "Gender": ["Male", "Female", "Male", "Female", "Male", "Female", "Male"],
    "Marks": [85, 80, 78, 'Nan', 76, 82, 'Nan'],
    "id": [120,121,122,123,124,125,126]
}
df1 = pd.DataFrame(data1)
print(df1)

data2 = {
    "Fee":[1000, 100000, 50000, 2000, 500, 70000, 30000],
       "id": [120,121,122,123,124,125,126]
}
df2 = pd.DataFrame(data2)
print(df)

df3 = pd.merge(df1, df2)
df3

df.loc[0, 'Marks']= 100
print(df)

df.plot.box()

df.plot.hist()

df['Gender'].plot.hist()



Decription:

Objective:
To perform data wrangling operations using Python on an open-source dataset, including data cleaning, transformation, and restructuring, to prepare the data for meaningful analysis and visualization.

Theory:
Data wrangling, also known as data munging, is the process of cleaning, transforming, and organizing raw data into a structured and usable format. It is a critical step in data analysis, as real-world data is often messy, incomplete, and inconsistent. Key steps in data wrangling include:

Data Collection - Acquiring data from various sources like CSV, Excel, databases, or APIs.

Data Cleaning - Handling missing values, removing duplicates, correcting data types, and managing outliers.

Data Transformation - Normalizing, aggregating, or converting data into the desired structure.

Feature Engineering - Creating new features or modifying existing ones to improve model performance.

Data Integration - Merging or joining multiple datasets for comprehensive analysis.

Python libraries like Pandas, NumPy, and OpenPyXL are widely used for these tasks, providing powerful tools for data manipulation and preprocessing.

Conclusion:
Through this practical, I developed a deeper understanding of the data wrangling process, including data cleaning, transformation, and feature engineering. This experience emphasized the importance of data quality in achieving accurate analysis and building robust machine learning models, highlighting the critical role of preprocessing in the data science workflow.


Explainantion:

import pandas as pd
import numpy as np
---pd is the alias for the Pandas library, used for data manipulation and analysis.
   np is the alias for the NumPy library, used for numerical computations.

data = {
    "Name": ["Amit", "Priya", "Raj", "Sneha", "Vikram", "Ananya", "Rohan"],
    "Gender": ["Male", "Female", "Male", "Female", "Male", "Female", "Male"],
    "Marks": [85, 80, 78, 'Nan', 76, 82, 'Nan'],
    "Age": [20,21,22,23,24,25,26]
}
df = pd.DataFrame(data)
print(df)
---Creates a dictionary data with Name, Gender, Marks, and Age as keys.
Converts this dictionary into a DataFrame using pd.DataFrame().
Prints the DataFrame.

df.mean()
---Calculates the mean of all numeric columns (Age and Marks) but does not store the result.

cat=[]
con=[]
for i in df.columns:
    if(df[i].dtypes=="object"):
        cat.append(i)
else:
    con.append(i)
df
---cat: List to store categorical columns.
con: List to store continuous (numerical) columns.
Loops through each column:
If the data type is object (i.e., text), it adds the column to cat.
Otherwise, adds it to con.
The else block is incorrectly placed here. It will always add the last column to con, even if it is object.

cat con ---Printing Cat Con

c=avg=sum=0
for ele in df['Marks']:
    if str(ele).isnumeric():
        c+=1
        sum+=ele
if c>0:
    avg=sum/c
df=df.replace(to_replace='Nan', value=avg)
df
---Initializes counters c, avg, and sum to zero.
Iterates through the Marks column:
Counts and sums only numeric values.
Calculates the average if at least one numeric value is present.
Replaces all instances of 'Nan' (string) with this calculated average.

df.isna().sum().plot(kind="bar")
---Checks for missing (NaN) values in each column and plots the result as a bar graph.
Useful for quickly visualizing which columns have missing data.

df['Gender']=df['Gender'].map({'Male':0,'Female':1}).astype(int)
df
---Replaces Male with 0 and Female with 1.
Converts the Gender column to int to make it a numerical feature.

df=df[df['Marks']>80]
df
---Filters the DataFrame to keep only rows where Marks are greater than 80.

df=df.drop(['Age'], axis=1)
df
---Removes the Age column from the DataFrame.
The axis=1 parameter specifies that a column (not a row) is being dropped.

data1 = {
    "Name": ["Amit", "Priya", "Raj", "Sneha", "Vikram", "Ananya", "Rohan"],
    "Gender": ["Male", "Female", "Male", "Female", "Male", "Female", "Male"],
    "Marks": [85, 80, 78, 'Nan', 76, 82, 'Nan'],
    "id": [120,121,122,123,124,125,126]
}
df1 = pd.DataFrame(data1)
print(df1)

data2 = {
    "Fee":[1000, 100000, 50000, 2000, 500, 70000, 30000],
    "id": [120,121,122,123,124,125,126]
}
df2 = pd.DataFrame(data2)
print(df)
---Creates df1 and df2 with unique IDs for each student.
df1 has Name, Gender, Marks, and id.
df2 has Fee and id.

df3 = pd.merge(df1, df2)
df3
---Merges df1 and df2 on the common id column, combining their information.

df.loc[0, 'Marks']= 100
print(df)
---Sets the Marks of the first row (index 0) to 100.

df.plot.box()
---Creates a box plot for the numeric columns in df to visualize data distribution and detect outliers.

df.plot.hist()
---Plots a histogram for all numeric columns, showing the frequency distribution.

df['Gender'].plot.hist()
---Plots a histogram specifically for the Gender column, which now contains 0s and 1s.

Questions:

1.What is Data Wrangling and why is it important in data science?
Data wrangling is the process of cleaning and transforming raw data into a usable format. It’s important because real-world data is often messy and needs to be structured for meaningful analysis or modeling.

2.Why did you use a DataFrame instead of a regular Python dictionary or list?
A DataFrame is more powerful than a dictionary or list because it allows you to work with structured data, perform complex operations, and handle missing values efficiently.

3.What are the differences between Series and DataFrame in Pandas?
A Series is a one-dimensional labeled array, while a DataFrame is a two-dimensional table with rows and columns.

4.Why did you use pd.DataFrame() to create the data structure?
I used pd.DataFrame() because it allows for easy manipulation and analysis of structured data (rows and columns), which is the standard format for most data tasks.

5.Why did you choose to use 'Nan' as a placeholder for missing values instead of NaN?
The 'Nan' value was used as a string placeholder to demonstrate how to handle missing or incorrect values. In practice, NaN is the proper placeholder for missing numeric data.

6.How did you handle missing values in the Marks column, and why is it important?
I replaced missing values in Marks with the column’s average. Handling missing data ensures accurate analysis and prevents errors in calculations or model training.

7.What is the purpose of df.replace() in this code?
df.replace() was used to replace the 'Nan' values with the computed average, ensuring no missing values remain in the data.

8.Why did you map Male and Female to 0 and 1? What are the advantages of this approach?
Mapping Male and Female to 0 and 1 converts the categorical data into numerical form, making it easier for machine learning algorithms to process.

9.Why did you drop the Age column in this practical?
The Age column was dropped because it wasn’t necessary for the specific analysis in this example. It could be retained if relevant to the problem.

10.What is the difference between df.dropna() and df.replace() for handling missing data?
df.dropna() removes rows with missing values, while df.replace() replaces missing values with a specified value, like the average.

11.Why did you use pd.merge() to combine df1 and df2?
pd.merge() was used to combine two DataFrames based on a common column (id), creating a new DataFrame that contains data from both.

12.What is the difference between categorical and continuous data?
Categorical data represents categories or labels (e.g., gender, color), while continuous data represents numerical values that can have a range of values (e.g., age, salary).

13.Why did you choose to replace the 'Nan' values in the Marks column with the average instead of just dropping them?
Replacing missing values with the average ensures we retain all data points, preserving the sample size, which is important for analysis and model training.

14.What is the impact of outliers on data analysis, and how can you handle them?
Outliers can skew analysis and model performance. They can be handled by removing them or transforming the data to minimize their impact.

15.Why is feature encoding (like converting Gender to 0/1) necessary in machine learning?
Machine learning algorithms work with numerical data, so encoding categorical features like Gender into numerical values is required for them to process the data.

16.What is the purpose of data visualization and why did you include a box plot and histogram in this practical?
Data visualization helps understand data distributions, patterns, and outliers. The box plot shows data spread and outliers, while the histogram displays frequency distributions.

17.How would you use this wrangled data for machine learning?
The cleaned and transformed data can be used to train machine learning models, where the features are ready for algorithms to predict outcomes (e.g., predicting a student’s performance based on their marks and gender).

18.Can you explain the significance of each step in the data wrangling process for real-world projects?
Data wrangling ensures that the data is clean, consistent, and structured, which is crucial for accurate analysis and decision-making in real-world projects.

19.How would you handle a scenario where the Marks column had a mix of strings and numbers?
You would first identify and clean the data by converting valid strings to numeric values and removing or handling invalid ones (e.g., replacing them with NaN or imputation).

20.Why is data quality important before applying machine learning algorithms?
Poor-quality data leads to inaccurate models. Clean, consistent, and well-structured data ensures better performance and more reliable predictions.

21.How can you automate this data wrangling process for repeated use?
You can write reusable functions or create pipelines using libraries like Pandas or Scikit-learn to automate the data wrangling process for new datasets.

