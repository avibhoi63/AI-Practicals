Aim: Create a Linear Regression model using python to predict home prise using Boston Housing dataset.

Codes:

import pandas as pd

data =pd.read_csv("C:/Users/nikhi/Downloads/HousingData.csv")

data

data.head(2)

data.info()

data.describe()

data
.corr()

X = data[["RM"]]
Y = data[["MEDV"]]

from seaborn import scatterplot
scatterplot(X.RM,Y.MEDV)

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(X,Y,test_size=0.2)

from sklearn.linear_model import LinearRegression
lm = LinearRegression()
model = lm.fit(xtrain,ytrain)

b1 = model.coef_

b0 = model.intercept_

b0 

b1

from seaborn import regplot
regplot(X,Y)

pred = model.predict(xtest)

from sklearn.metrics import mean_absolute_error
mean_absolute_error(ytest,pred)

from sklearn.metrics import mean_squared_error
mean_squared_error(ytest,pred)

print(ytest)

print(pred)


Description:

Objective:
To understand the principles of Linear Regression.
To explore the Boston Housing dataset.
To preprocess the data for better model performance.
To train, test, and evaluate the Linear Regression model.
To make predictions and assess the accuracy of the model.

Theory:
Linear Regression is a supervised learning algorithm used for predicting a continuous target variable based on one or more input features. It attempts to model the relationship between the independent variables (features) and the dependent variable (target) by fitting a straight line (or hyperplane in multiple dimensions) that best represents the data.
The equation for Simple Linear Regression is:
ùëå = ùëè,0 + ùëè,1 ùëã + ùúñ
Where:
Y = Predicted output (house price)
b_0 = Intercept (where the line crosses the Y-axis)
b_1 = Coefficient (slope of the line)
X = Input feature
Œµ = Error term (residual)

In Multiple Linear Regression, the equation extends to:
Y= b,0 + b,1 X,1 + b,2 X,2 + ‚ãØ + b,n X,n + œµ

Key Steps in Building the Model:
Data Collection - Load the Boston Housing dataset.
Data Preprocessing - Handle missing values, normalize features, and split the data.
Model Training - Train the Linear Regression model on the training set.
Model Evaluation - Test the model‚Äôs accuracy using metrics like R-squared and Mean Squared Error (MSE).
Prediction - Use the trained model to predict house prices.
Visualization - Plot the results to assess model performance.

Conclusion:
Through this practical, I developed a deeper understanding of how Linear Regression works and its applications in real-world prediction problems. I successfully built and evaluated a model that predicts house prices based on various features, highlighting the importance of data preprocessing and feature selection for accurate predictions. This exercise reinforced the critical role of regression in data science and machine learning.


Explaination:

import pandas as pd---Imports the Pandas library for data manipulation and analysis.
                      pd is the common alias for Pandas.

data = pd.read_csv("C:/Users/nikhi/Downloads/HousingData.csv")
---Reads the HousingData.csv file from the specified path into a Pandas DataFrame named data.
   Assumes the file is in CSV format.

data---Display the DataFrame

data.head(2)---hows the first 2 rows of the dataset to get a quick preview of the data.

data.info()---OVerview of dataframe

data.describe()---Generates summary statistics

data.corr()---Calculates the correlation matrix to show the relationship between each pair of columns.

X = data[["RM"]]
Y = data[["MEDV"]]
---Sets X to the RM (average number of rooms per dwelling) column, which will be the input feature.
   Sets Y to the MEDV (Median value of owner-occupied homes) column, which will be the target variable.

from seaborn import scatterplot
scatterplot(X.RM,Y.MEDV)---Creates a scatter plot to visualize the relationship between RM (number of rooms) and MEDV (house price).

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2)
---Splits the data into training and testing sets.
   80% for training, 20% for testing by default.
   Randomly selects data, ensuring the model is evaluated on unseen data.

from sklearn.linear_model import LinearRegression
lm = LinearRegression()
model = lm.fit(xtrain,ytrain)---Creates a LinearRegression model object (lm)

b1 = model.coef_
---Retrieves the intercept (constant) of the linear regression line.

b0 ---Prints the intercept of the model.

b1---Prints the coefficient of the model, indicating how much MEDV changes for a one-unit change in RM

from seaborn import regplot
regplot(X, Y)---Creates a regression plot to visualize the fitted line over the original data points.

from sklearn.metrics import mean_absolute_error
mean_absolute_error(ytest, pred)
---Calculates the Mean Absolute Error (MAE), which measures the average absolute difference between the actual and predicted values.
   Lower values indicate better model performance.

from sklearn.metrics import mean_squared_error
mean_squared_error(ytest, pred)---Calculates the Mean Squared Error (MSE), which measures the average squared difference between the actual and predicted values.
                                  More sensitive to outliers than MAE.

print(ytest)---Prints the actual values of MEDV from the test set.

print(pred)---Prints the predicted values of MEDV from the model.



Questions:

What is Linear Regression?
Linear Regression predicts a continuous target variable using one or more input features by fitting a straight line.

Difference between Simple and Multiple Linear Regression?
Simple has one feature (e.g., RM), while Multiple has two or more features.

Why use RM as the feature?
RM (average rooms) is highly correlated with MEDV (house price), making it a strong predictor.

Why split the data?
To evaluate the model on unseen data and prevent overfitting.

Purpose of train_test_split?
To divide the data into training and testing sets for fair model evaluation.

Why use pd.read_csv()?
To read the CSV file into a Pandas DataFrame for easier data manipulation.

Purpose of data.info()?
To check data types, non-null counts, and memory usage.

Why check data.describe()?
To understand the statistical summary of each column, like mean, min, and max values.

Why use data.corr()?
To find correlations between features, helping select the best predictor.

Handling missing values?
Replace them, drop rows, or use imputation depending on the context.

What does fit() do?
It trains the model using the training data.

Role of b0 (intercept) and b1 (coefficient)?
b0 is the starting point of the line, b1 is the slope indicating the impact of RM on MEDV.

Difference between scatterplot() and regplot()?
scatterplot shows data points, regplot adds the regression line.

Why visualize X and Y first?
To confirm a linear relationship exists, which is crucial for linear regression.

Interpret a negative coefficient?
It means the target variable decreases as the feature value increases.

Why MAE and MSE?
MAE measures average error, MSE penalizes larger errors more, providing a different perspective on model accuracy.

Difference between MAE and MSE?
MSE is more sensitive to outliers because it squares the errors.

Overfitting vs Underfitting?
Overfitting means the model is too complex, underfitting means it's too simple.

What is R-squared?
It measures the proportion of variance explained by the model, typically included for deeper evaluation.

Improving accuracy?
Use multiple features, remove outliers, or try regularization.

Why normalize or standardize?
To avoid features with larger scales dominating the model.

Extend to Multiple Linear Regression?
Add more features, like LSTAT or TAX, to the X matrix.

Non-linear relationship handling?
Use polynomial features or non-linear models like decision trees.

Handling multicollinearity?
Remove correlated features or use Ridge or Lasso regression.

Real-world use of this model?
For real estate pricing, market analysis, or investment decisions.

Optimizing for large datasets?
Use libraries like Dask or Spark for distributed processing.

Why Seaborn over Matplotlib?
Seaborn has simpler syntax for statistical plots and better aesthetics.

Role of model.predict()?
Generates predictions based on the model's learned parameters.

Why check for heteroscedasticity?
To confirm the model's errors are consistent across all predictions.

Automating this process?
Use a pipeline or deploy the model as an API for continuous predictions.
